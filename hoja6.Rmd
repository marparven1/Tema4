---
title: "Hoja 6 de problemas y prácticas con R"
subtitle: "Estadística Computacional I. Grado en Estadística Departamento de Estadística e Investigación Operativa"
author: "Marta Venegas Pardo"
output:
    pdf_document: 
    toc: yes
    toc_depth: 3
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




# Ejercicio 1
1. Responder a los siguientes apartados:

```{r}
library(randtoolbox)
```

Tiene muchas funciones relacionadas con el tema.

## Generar secuencias de tamaño 500 según los generadores de Mersenne-Twister, Congruencia lineal, “Knuth-TAOCP-2002” y una secuencia determinista.

### Generar de congruencia lineal y algunos contrastes
```{r}
library(tseries) # Para contrastes de independencia
```


```{r}
n<-500
set.seed(1)
(head (xmt<- runif(n) , 10 )) #Mersenne-Twister
```


```{r}
(head (xcl<- congruRand(n),10)) #Generador de congruencia lineal
```


```{r}
(head(xta<-knuthTAOCP(n),10)) #Knuth-TAOCP-2002
```


```{r}
(head(xdet<- ((1:n)-0.5)/n,10))
```



## Dibujar gráficos de líneas e histogramas para las cuatro secuencias. 

### Gráficos de líneas con plot

```{r}
par(mfrow=c(2,2))
plot(xmt,type="l",main="Mersenne-Twister")
plot(xcl,type="l",main="Congruencia lineal")
plot(xta,type="l",main="Knuth-TAOCP-2002")
plot(xdet,type="l",main="Sec.Determinista")
```


### Histogramas con hist

```{r}
par(mfrow=c(2,2))
hist(xmt,br=20,main="Mersenne-Twister")
hist(xcl,br=20,main="Congruencia lineal")
hist(xta,br=20,main="Knuth-TAOCP-2002")
hist(xdet,br=20,main="Sec.Determinista")

par(mfrow=c(1,1))
```


## Aplicar contrastes de aleatoriedad sobre las cuatro secuencias.

```{r}
contrastes_aleato<-function(x,titulo) {
print(titulo)
ks.test(x, "punif")
gap.test(x) #Por defecto, l=0, u=0.5
order.test(x,d=5) #d puede ser 2,3,4,5, pero n debe ser múltiplo de d freq.test(x, 1:4)#Por defecto, secuencia 1:15
serial.test(x,d=5) #n debe ser múltiplo de d (t=2)
poker.test(x)
print(runs.test(factor(x>0.5)))
acf(x,main=titulo)
}
```


### Mersenne-Twister

```{r}
contrastes_aleato(xmt,"Mersenne-Twister")
```


No se sale mucho de las líneas, para entender las autocorreaciones.

### Congruencia lineal
```{r}
contrastes_aleato(xmt,"Congruencia Lineal")
```

Los p-valores no están tan al límite como alguno de los anteriores.

### Knuth-TAOCP-2002

```{r}
contrastes_aleato(xta,"Knuth-TAOCP-2002")
```

### Determinista

```{r}
contrastes_aleato(xdet,"Determinista")
```



### Test con generadores (no secuencias)
Le pasamos la función generadora y hará un estudio.

```{r}
coll.test(runif,2^7,2^10)
```

p-valor alto, no rechazo.


```{r}
#m=2^10 secuencias de tamaño n=2^7 
##por defecto tdim=2 
coll.test(congruRand,2^7,2^10)
```
```{r}
coll.test(knuthTAOCP,2^7,2^10)
```


# Ejercicio 2

Comparar empíricamente los rendimientos de los siguientes generadores de números aleatorios en (0,1), Mersenne-Twister, Congruencia Lineal, K2nuth-TAOCP-2002,2 además de un generador de números determinista, según los resultados del test de Kolmogorov-Smirnov y el test de huecos.

Utilizar 1000 muestras de tamaño 100 y analizar de forma numérica y gráfica los resultados.


```{r}
library(randtoolbox)
M<-1000
n<-100
pvaloresKS<- matrix(NA,M,4)
pvaloresGap<- matrix(NA,M,4)
colnames(pvaloresKS)<- c("Mersenne-Twister","Congruencia Lineal", "Knuth-TAOCP-2002","Determinista")
colnames(pvaloresGap)<- c("Mersenne-Twister","Congruencia Lineal", "Knuth-TAOCP-2002","Determinista")

for (i in 1:M) {
if (i%%50==0) {
cat("Muestra ",i,"de ",M,"\n")
}
xmt<- runif(n) #Mersenne-Twister
xcl<- congruRand(n) #Generador de congruencia lineal
xta<-knuthTAOCP(n) #Knuth-TAOCP-2002
xdet<- ((1:n)-0.5)/n #Secuencia determinista
pvaloresKS[i,]<- c(ks.test(xmt, "punif")$p.value,
                   ks.test(xcl, "punif")$p.value, 
                   ks.test(xta, "punif")$p.value,
                   ks.test(xdet, "punif")$p.value)
pvaloresGap[i,]<- c(gap.test(xmt,echo=FALSE)$p.value,
                    gap.test(xcl,echo=FALSE)$p.value,
                    gap.test(xta,echo=FALSE)$p.value,
                    gap.test(xdet,echo=FALSE)$p.value)
}
```

```{r}
head(pvaloresKS,10)
```


```{r}
head(pvaloresGap,10)
```

```{r}
#Ejercicio: dibujar en una sola pantalla los M p-valores KS y Gap para los #cuatro generadores
```



```{r}
 par(mfrow=c(2,2)) 
for (i in 1:4)
{
plot(pvaloresKS[,i],type="l",
     main=colnames(pvaloresKS)[i], 
     xlab="Muestra",ylab="p-valor",
     col="red",lwd=2,ylim=c(0,1))
lines(pvaloresGap[,i], col="blue",lwd=2) 
}
legend("center",col=c("red","blue"),lty=1,
       lwd=2,legend=c("KS","Gap"))
```
```{r}
par(mfrow=c(1,1))
```



## Estimar P[p<alfa]

```{r}
alfa<- 0.05
rendim<- matrix(NA,4,2)
colnames(rendim)<- c("KS","Gap")
rownames(rendim)<- c("Mersenne-Twister","Congruencia Lineal", "Knuth-TAOCP-2002","Determinista")
for (i in 1:4)
rendim[i,]<-c(mean(pvaloresKS[,i]<=alfa), 
              mean(pvaloresGap[,i]<=alfa)) 
#Ejercicio: completar
rendim
```


```{r}
plot(rendim,type="n", 
     main=expression(hat(P)(pv<=alpha)))
text(rendim,label=rownames(rendim),cex=0.7) 
grid()
```

### Cuántas veces es mayor un pv que el otro

```{r}
for (i in 1:4) 
  cat(colnames(pvaloresKS)[i],mean(pvaloresKS[,i]>pvaloresGap[,i]),"\n")
```


# Ejercicio 3 

## Teorema central del límite

3. Ilustrar el Teorema Central del Límite con muestras de una ley Geométrica con parámetro 0.1 para tamaños muestrales 3, 10, 25, 50 y número de muestras 5000.

```{r}
M<-5000 #Núm. de muestras
p<-0.1
vn<-c(3,10,25,50) #Tamaños de cada muestra 
paranti<- par(no.readonly=T)
par(mfrow=c(2,2))
for (n in vn) #Ejercicio: completar
{ x<- matrix(rgeom(M*n,p), # Cuantas genero (5000*3), con parametro p=0.1
             nrow=M,ncol=n) 
  medias<- rowMeans(x) 
  hist(medias,freq=FALSE, col="blue",
     main=paste("n =",n),
     ylab=expression(f[i] /a[i]))
}
```

```{r}
par(paranti)
```


# Ejercicio 4

## Intervalo de confianza con una normal

4. Ilustrar el concepto de intervalo de confianza mediante 100 muestras de tamaño 10 de una ley N(0,1), siendo la media poblacional el parámetro de interés.

```{r}
n<-10 #Tamaño muestral 
M<-100 #M muestras 
alfa<-0.05
media<-0
desvtip<-1
muestras<-matrix(rnorm(M*n,mean=media,sd=desvtip),
                 nrow=M,ncol=n,byrow=TRUE)
```


```{r}
pv<-array(0,M) #M pvalores 
exinf<-numeric(M) 
exsup<-numeric(M)
```



```{r}
for (i in 1:M)
{ #Ejercicio: completar
  test<-t.test(muestras[i,],conf.level=1-alfa) 
  exinf[i]<-test$conf.int[1] 
  exsup[i]<-test$conf.int[2]
}

plot(gl(M,n),as.vector(t(muestras)),main=paste("100 I.C. 95% ","n=",n), xlab="Muestra", ylab="x",col="lightblue")
abline(h=media,col="red",lwd=2,lty=2)
lines(exinf,col="darkblue",lwd=4)
lines(exsup,col="darkblue",lwd=4)
```

```{r}
cat("Cobertura observada =",
100* mean((exinf<= media) & (exsup>=media)), "% \n")
```

```{r}
cat("Longitud media =", mean(exsup-exinf),"\n")
```


# Ejercicio 5

5. Escribir y probar una función R para generar muestras de una Weibull:

\[ f(t) = \lambda \alpha(\lambda t)^{\alpha -1} e ^{- (\lambda t)} \text{ , } \space F(t)=1 - e^{-(\lambda t)^\alpha} \text{ , } \lambda , \alpha >0
\]

## Generar muestras de tamaño 200 para las configuraciones siguientes, donde:

- $\lambda$ es el parámetro de escala
- $\alpha$ es el parámetro de forma

- $\alpha=0.5 \text{ , } \lambda = 1$
- $\alpha=1 \text{ , } \lambda = 1$
- $\alpha=2 \text{ , } \lambda = 1$
- $\alpha=2 \text{ , } \lambda = 3$



```{r}
generaweib<- function(n,alfa,landa){
  U<- runif(n) 
  ((-log(1-U))^(1/alfa))/landa
}
#i)
n<-200
set.seed(129871)
x1<- generaweib(n,0.5,1) 
x2<- generaweib(n,1,1) 
x3<- generaweib(n,2,1) 
x4<- generaweib(n,2,3)
```


## Dibujar los histogramas

```{r}
tit1<-paste("alpha=",0.5,"lambda=",1) 
tit2<-paste("alpha=",1,"lambda=",1) 
tit3<-paste("alpha=",2,"lambda=",1) 
tit4<-paste("alpha=",2,"lambda=",3)

par(mfrow=c(2,2))
hist(x1,main=tit1,br=20,prob=TRUE)
hist(x2,main=tit2,br=20,prob=TRUE)
hist(x3,main=tit3,br=20,prob=TRUE)
hist(x4,main=tit4,br=20,prob=TRUE)
```

```{r}
par(mfrow=c(1,1))
```

## Representar las funciones de distribución empírica y superponer las teóricas


```{r}
par(mfrow=c(2,2))
plot(ecdf(x1),main=tit1,do.points=FALSE,verticals=TRUE)
curve(pweibull(x,0.5,1),min(x1),max(x1),1000,add=TRUE,col="red")

# ecdf: EMpirical cumulative distribution Function

plot(ecdf(x2),main=tit2,do.points=FALSE,verticals=TRUE)
curve(pweibull(x,1,1),min(x2),max(x2),1000,add=TRUE,col="red")

plot(ecdf(x3),main=tit3,do.points=FALSE,verticals=TRUE)
curve(pweibull(x,2,1),min(x3),max(x3),1000,add=TRUE,col="red")

plot(ecdf(x4),main=tit4,do.points=FALSE,verticals=TRUE)
curve(pweibull(x,2,1/3),min(x4),max(x4),1000,add=TRUE,col="red")
```
```{r}
par(mfrow=c(1,1))
```





## Contrastes de bondad de ajuste (fitdistrplus)

SIGUE UNA WEIBULL FRENTE A NO. ESCRIBIR.

Debemos estimar los parámetros por el método de la máxima verosimilitud.

```{r message=FALSE}
library(fitdistrplus)
```

```{r}
print(mv1<-mledist(x1,"weibull"))
```

### Test de Kolmogorov-Smirnov de bondad de ajuste

Es el mejor.


```{r}
ks.test(x1,"pweibull",mv1$estimate[1], 1/mv1$estimate[2]) 
#R usa 1/landa
```
Cuidado al dar los parámetros.

Conclusión, p-valor mayor que alpha, no tenemos evidencias para negar que la muestra x1 siga una distribución pweibull.


Otra opción: ajustar los datos mediante fitdist y luego aplicar gofstat: Aquí lo que se muestra es el p-valor del test chi-cuadrado de bondad de ajuste.

```{r}
fitweib1 <- fitdist(x1, "weibull") 
summary(fitweib1)
```

Obtenemos los estimadores, que coinciden con los anteriores.
También calcula el log de la verosimilitud y calcula los criterios AIC y BIC.

### MEDIANTE el test Chi-Cuadrado

```{r}
gofstat(fitweib1)$chisqpvalue
```

Acepto H0, no encontramos evidencias de que no la sigue.

```{r}
fitweib2 <- fitdist(x2, "weibull") 
summary(fitweib2)
```

```{r}
gofstat(fitweib2)$chisqpvalue
```


```{r}
fitweib3 <- fitdist(x3, "weibull")
summary(fitweib3)
```


```{r}
gofstat(fitweib3)$chisqpvalue
```



```{r}
fitweib4 <- fitdist(x4, "weibull")
summary(fitweib4)
```



```{r}
gofstat(fitweib4)$chisqpvalue
```


# Ejercicio 6 "Pesos.RData"

Leer el fichero datos en “Pesos.RData”, y a continuación:

## Estimar la densidad por el método del núcleo.

```{r}
load("datos/Pesos.RData")
hist(datos,
     br=30,
     prob=TRUE,
     main="Histograma y estimación de la densidad",
     ylab = expression(hat(f)(x)),xlab="x")
lines(density(datos,bw="SJ"),col="blue",lwd=2) # Estimación por el método del núcleo con density
## SJ es el método que mejor se porta
```


Vamos a crear una muestra de datos a partir de la función de densidad estimada, serán estimaciones.

```{r}
estimnuc<- density(datos,bw="SJ",n=5000) #Para tener más puntos 
# Devuelve los puntos sobre los que ha hecho las estiamciones

distrib<-data.frame(x=estimnuc$x,
                    F=cumsum(estimnuc$y)/sum(estimnuc$y))
# Aquí tenemos una tabla de valores de la función de distribución, no de densidad

plot(distrib,type="l") # Función de distribución 
```


## Escribir una función para generar valores según dicha densidad estimada.

```{r}
generax<- function(n,distrib) {
  U<-runif(n)
  sapply(U, function(u) min(distrib$x[distrib$F>=u]) ) }
## Mínimo x que supera al valor que busco en la FDD
## Función inversa de la función tabulada
## Sapply: acada elemento del vector que le paso, le aplica la función
generax(10,distrib)
```


Simulación de valores:

```{r}
xsimu<- generax(200,distrib)
```
Lo dibujo


```{r}
qqplot(datos,xsimu)
```



```{r}
summary(datos)
```

```{r}
summary(xsimu)
```

Vemos que la mediana con el segundo método es mayor, al igual que la media. Sin embardo, estas evidencias no son significativas para rechazar el test que realizamos a continuación:

## Comparar las distribuciones de una muestra generada de tamaño 200 y el conjunto de datos original.

```{r}
ks.test(datos,xsimu)
```
```{r}
plot(ecdf(datos),main="",do.points=FALSE, verticals=TRUE,col="red")
lines(ecdf(xsimu),do.points=FALSE, verticals=TRUE,col="blue")
```

Aquí vemos porque K-S no rechaza.

# Ejercicio 7

7. Diseñar una función para generar realizaciones de una ley Geométrica simulando el proceso de conteo del número de fracasos antes del primer éxito en la repetición de ensayos Bernouilli. Probar la función y analizar los resultados.

```{r}
#para generar muestra de tamaño n de Ge(p)
#
Geom<- function(n,p) {
 #Inicializaciones
X<- integer(n) 
#algoritmo
for (i in 1:n) {
s<- -1
repeat
{
s<- s+1
U<- runif(1)
if (U <= p) break
}
X[i]<- s
}
X
}
```


```{r}
p<- 0.3
n<- 2000
set.seed(12345)
x<- Geom(n,p)
(tabla<-table(x))
```


```{r}
vx<- 1:max(x)
plot(prop.table(tabla),
     type="h",
     ylab="",
     main=paste("p=",p,"n=",n))
lines(vx,dgeom(vx,p),type="h",col="red",lty=2)
legend("topright",
       lty=1:2,
       col=c("black","red"),
       legend=c("Frec.relativa","Prob.Geom"))
grid()
```

No poner valores de p muy pequeños.



```{r}
plot(ecdf(x),
     do.points=FALSE,
     verticals=TRUE, 
     ylab="",
     main=paste("p=",p,"n=",n))

curve(pgeom(x,p),
      add=TRUE,
      col="red",
      lty=2) 
legend("center",
       lty=1:2,
       col=c("black","red"),
       legend=expression(F[n](x),F[Geom](x)))
grid()
```

Fin hoja

